{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 키워드 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>getPhraseText</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>two nil win win.</td>\n",
       "      <td>00:00:01</td>\n",
       "      <td>00:00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Son scored the first goal first.</td>\n",
       "      <td>00:00:02</td>\n",
       "      <td>00:00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Got the new stadium.</td>\n",
       "      <td>00:00:03</td>\n",
       "      <td>00:00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Did you enjoy that?</td>\n",
       "      <td>00:00:04</td>\n",
       "      <td>00:00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not, you know, even when I didn't score first ...</td>\n",
       "      <td>00:00:06</td>\n",
       "      <td>00:00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Really?</td>\n",
       "      <td>00:00:10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Really?</td>\n",
       "      <td>00:00:11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Really.</td>\n",
       "      <td>00:00:11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ah, this evening and and of course, ah school ...</td>\n",
       "      <td>00:00:12</td>\n",
       "      <td>00:00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>But I think the most important thing, it's ah ...</td>\n",
       "      <td>00:00:17</td>\n",
       "      <td>00:00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What's the noise like?</td>\n",
       "      <td>00:00:22</td>\n",
       "      <td>00:00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ah, amazing.</td>\n",
       "      <td>00:00:25</td>\n",
       "      <td>00:00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Amazing.</td>\n",
       "      <td>00:00:27</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I mean, you can't even believe you know when y...</td>\n",
       "      <td>00:00:27</td>\n",
       "      <td>00:00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>You know, I think it was, I think was allowed ...</td>\n",
       "      <td>00:00:39</td>\n",
       "      <td>00:00:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ah feeling.</td>\n",
       "      <td>00:00:48</td>\n",
       "      <td>00:00:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>And that's why I think we put him even more ar...</td>\n",
       "      <td>00:00:49</td>\n",
       "      <td>00:00:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A lot of the fans say they feel like they've c...</td>\n",
       "      <td>00:00:55</td>\n",
       "      <td>00:00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Yeah, of course.</td>\n",
       "      <td>00:00:58</td>\n",
       "      <td>00:00:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        getPhraseText start_time  end_time\n",
       "0                                    two nil win win.   00:00:01  00:00:02\n",
       "1                    Son scored the first goal first.   00:00:02  00:00:03\n",
       "2                                Got the new stadium.   00:00:03  00:00:04\n",
       "3                                 Did you enjoy that?   00:00:04  00:00:05\n",
       "4   Not, you know, even when I didn't score first ...   00:00:06  00:00:10\n",
       "5                                             Really?   00:00:10       NaN\n",
       "6                                             Really?   00:00:11       NaN\n",
       "7                                             Really.   00:00:11       NaN\n",
       "8   Ah, this evening and and of course, ah school ...   00:00:12  00:00:17\n",
       "9   But I think the most important thing, it's ah ...   00:00:17  00:00:22\n",
       "10                             What's the noise like?   00:00:22  00:00:23\n",
       "11                                       Ah, amazing.   00:00:25  00:00:26\n",
       "12                                           Amazing.   00:00:27       NaN\n",
       "13  I mean, you can't even believe you know when y...   00:00:27  00:00:39\n",
       "14  You know, I think it was, I think was allowed ...   00:00:39  00:00:48\n",
       "15                                        Ah feeling.   00:00:48  00:00:49\n",
       "16  And that's why I think we put him even more ar...   00:00:49  00:00:54\n",
       "17  A lot of the fans say they feel like they've c...   00:00:55  00:00:58\n",
       "18                                   Yeah, of course.   00:00:58  00:00:59"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff=pd.read_csv(\"video_us2.csv\")\n",
    "ff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 텍스트를 문장으로 이루어진 리스트로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['two nil win win.',\n",
       " 'Son scored the first goal first.',\n",
       " 'Got the new stadium.',\n",
       " 'Did you enjoy that?',\n",
       " \"Not, you know, even when I didn't score first gold probably are injured.\",\n",
       " 'Really?',\n",
       " 'Really?',\n",
       " 'Really.',\n",
       " 'Ah, this evening and and of course, ah school first called is also important.',\n",
       " \"But I think the most important thing, it's ah because three points here in unbelievable stadium.\",\n",
       " \"What's the noise like?\",\n",
       " 'Ah, amazing.',\n",
       " 'Amazing.',\n",
       " \"I mean, you can't even believe you know when you're when you're on the pitcher and the bounce back And when the when the cross start Ah, um, make the noise and you know, the balance back on the pitch and then to the to the stand again.\",\n",
       " 'You know, I think it was, I think was allowed this Ah ah, when I played it, you know, anything was just unbelievable.',\n",
       " 'Ah feeling.',\n",
       " \"And that's why I think we put him even more are better than few last few games ago.\",\n",
       " \"A lot of the fans say they feel like they've come home to the players feel like that.\",\n",
       " 'Yeah, of course.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sens=[]\n",
    "for ffs in ff.getPhraseText:\n",
    "    sens.append(ffs)\n",
    "sens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한 리스트로 합친 후 소문자로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"two nil win win.son scored the first goal first.got the new stadium.did you enjoy that?not, you know, even when i didn't score first gold probably are injured.really?really?really.ah, this evening and and of course, ah school first called is also important.but i think the most important thing, it's ah because three points here in unbelievable stadium.what's the noise like?ah, amazing.amazing.i mean, you can't even believe you know when you're when you're on the pitcher and the bounce back and when the when the cross start ah, um, make the noise and you know, the balance back on the pitch and then to the to the stand again.you know, i think it was, i think was allowed this ah ah, when i played it, you know, anything was just unbelievable.ah feeling.and that's why i think we put him even more are better than few last few games ago.a lot of the fans say they feel like they've come home to the players feel like that.yeah, of course.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_list=''.join(sens)\n",
    "one_lower=one_list.lower()\n",
    "one_lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 특수문자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'two nil win win son scored the first goal first got the new stadium did you enjoy that not  you know  even when i didn t score first gold probably are injured really really really ah  this evening and and of course  ah school first called is also important but i think the most important thing  it s ah because three points here in unbelievable stadium what s the noise like ah  amazing amazing i mean  you can t even believe you know when you re when you re on the pitcher and the bounce back and when the when the cross start ah  um  make the noise and you know  the balance back on the pitch and then to the to the stand again you know  i think it was  i think was allowed this ah ah  when i playe'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규표현식을 사용해서 특수문자를 제거\n",
    "import re\n",
    "# 소문자와 대문자가 아닌 것은 공백으로 대체한다.\n",
    "letters_only = re.sub('[^a-zA-Z]', ' ', one_lower)\n",
    "letters_only[:700]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 토큰화(문자 나누기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['two', 'nil', 'win', 'win', 'son', 'scored', 'the', 'first', 'goal', 'first']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=letters_only.split()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 텍스트 전처기(NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['two',\n",
       " 'nil',\n",
       " 'win',\n",
       " 'win',\n",
       " 'son',\n",
       " 'scored',\n",
       " 'first',\n",
       " 'goal',\n",
       " 'first',\n",
       " 'got',\n",
       " 'new',\n",
       " 'stadium',\n",
       " 'enjoy',\n",
       " 'know',\n",
       " 'even',\n",
       " 'score',\n",
       " 'first',\n",
       " 'gold',\n",
       " 'probably',\n",
       " 'injured']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=[w for w in words if not w in stopwords.words('english')]\n",
    "words[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 핵심단어 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two nil win win son scored the first goal first got the new stadium\n",
      "did you enjoy that not  \n",
      "you know  \n",
      "even when i didn t score\n",
      "first gold probably are injured really really really ah  this evening\n",
      "and and of course  ah school first called is also important\n",
      "but i think the most important thing  \n",
      "it s ah because three points here in unbelievable stadium what s the noise like ah  \n",
      "amazing amazing\n",
      "i mean  \n",
      "you can t even believe you know when you re when you re on the pitcher and the bounce back and when the when the cross start ah  um  make the noise\n",
      "and you know  the balance back on the pitch\n",
      "and then to the to the stand again\n",
      "you know  \n",
      "i think it was  \n",
      "i think was allowed this ah ah  when i played it  \n",
      "you know  anything was just unbelievable\n",
      "ah feeling and\n",
      "that s why i think we put him even more\n",
      "are better than few last few games ago\n",
      "a lot of the fans say they feel like they ve come home to the players feel like that\n",
      "yeah  \n",
      "of course\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "doc=nlp(letters_only)\n",
    "for sents in doc.sents:\n",
    "    print(sents.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nil, win, win, son, scored, goal, got, stadium], [enjoy], [know], [didn, t, score], [gold, injured, evening], [course, school, called], [think, thing], [s, points, stadium, s, noise], [], [mean], [t, believe, know, pitcher, bounce, cross, start, noise], [know, balance, pitch], [stand], [know], [think], [think, allowed, played], [know], [feeling], [s, think], [games], [lot, fans, feel, ve, come, players, feel], [], [course]]\n"
     ]
    }
   ],
   "source": [
    "candidate_pos = ['NOUN', 'PROPN', 'VERB']\n",
    "sentences = []\n",
    "\n",
    "for sent in doc.sents:\n",
    "    selected_words = []\n",
    "    for token in sent:\n",
    "        if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "            selected_words.append(token)\n",
    "    sentences.append(selected_words)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "class TextRank4Keyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 10 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "    \n",
    "    def set_stopwords(self, stopwords):  \n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "    \n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "        \n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "        \n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "            \n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "        \n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "        \n",
    "        return g_norm\n",
    "\n",
    "    \n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            print(key + ' - ' + str(value))\n",
    "            if i > number:\n",
    "                break\n",
    "        \n",
    "        \n",
    "    def analyze(self, text, \n",
    "                candidate_pos=['NOUN', 'PROPN'], \n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "        \n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "        \n",
    "        # Pare text by spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "        \n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        \n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "        \n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        \n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        \n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "        \n",
    "        self.node_weight = node_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win - 1.2443749999999998\n",
      "noise - 1.1275\n",
      "Son - 1.0\n",
      "goal - 1.0\n",
      "stadium - 1.0\n",
      "that?Not - 1.0\n",
      "gold - 1.0\n",
      "evening - 1.0\n",
      "course - 1.0\n",
      "school - 1.0\n",
      "thing - 1.0\n",
      "points - 1.0\n"
     ]
    }
   ],
   "source": [
    "tr4w = TextRank4Keyword()\n",
    "tr4w.analyze(one_list, candidate_pos = ['NOUN', 'PROPN'], window_size=4, lower=False)\n",
    "tr4w.get_keywords(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
